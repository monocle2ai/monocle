import logging
import uuid
from operator import itemgetter

import bs4
import pytest
from common.custom_exporter import CustomConsoleSpanExporter
from langchain.prompts import ChatPromptTemplate
from langchain_chroma import Chroma
from langchain_community.document_loaders import WebBaseLoader
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from monocle_apptrace.instrumentation.common.instrumentor import (
    set_context_properties,
    setup_monocle_telemetry,
)
from monocle_apptrace.instrumentation.common.utils import logger
from opentelemetry.sdk.trace.export import BatchSpanProcessor

logger = logging.getLogger(__name__)


@pytest.fixture(scope="module")
def setup():
    custom_exporter = CustomConsoleSpanExporter()
    try:
        instrumentor = setup_monocle_telemetry(
            workflow_name="ashokan_rag_step_back",
            span_processors=[BatchSpanProcessor(custom_exporter, max_queue_size=1, max_export_batch_size=1)],
            wrapper_methods=[])

        set_context_properties({"session_id": f"{uuid.uuid4().hex}"})
        yield custom_exporter
    finally:
        # Clean up instrumentor to avoid global state leakage
        if instrumentor and instrumentor.is_instrumented_by_opentelemetry:
            instrumentor.uninstrument()


def test_langchain_rag_step_back(setup):
    # Load, chunk and index the contents of the blog.
    loader = WebBaseLoader(
        web_paths=("https://lilianweng.github.io/posts/2023-06-23-agent/",),
        bs_kwargs=dict(
            parse_only=bs4.SoupStrainer(
                class_=("post-content", "post-title", "post-header")
            )
        ),
    )
    docs = loader.load()

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    splits = text_splitter.split_documents(docs)
    vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())

    # Retrieve and generate using the relevant snippets of the blog.
    retriever = vectorstore.as_retriever()
    logger.info(f"retriever tags:{retriever.tags}")

    # STEP 1: Generate a step-back question (more abstract/broader)
    step_back_prompt = ChatPromptTemplate.from_template(
        """Given a specific question, create a more abstract, broader question that addresses 
        the underlying concepts or principles.
        
        Original question: {question}
        
        Step-back question (broader and more abstract):"""
    )

    # Remove spans generated by OpenAIEmbeddings() during vector operations
    setup.reset()
    
    # Original question
    original_question = "What happens to the pressure P, of an ideal gas if the temperature T is increased by a factor of 2 and the volume V is increased by a factor of 8?"
    llm = ChatOpenAI(model="gpt-3.5-turbo-0125", temperature=0.7)

    step_back_chain = step_back_prompt | llm | StrOutputParser()
    step_back_question = step_back_chain.invoke({"question": original_question})
    #logger.info(f"Step-Back Question: {step_back_question}")
    #logger.info("=" * 60)

    # STEP 2: Retrieve relevant documents for the step-back question
    # (In real implementation, this would use vector similarity search)
    # def simple_retrieval(query, docs, top_k=3):
    #     """Simple keyword-based retrieval for demonstration"""
    #     relevant_docs = []
    #     query_words = query.lower().split()
        
    #     for doc in docs:
    #         score = sum(1 for word in query_words if word in doc.lower())
    #         if score > 0:
    #             relevant_docs.append((doc, score))
        
    #     # Sort by relevance and return top_k
    #     relevant_docs.sort(key=lambda x: x[1], reverse=True)
    #     return [doc for doc, _ in relevant_docs[:top_k]]
    
    # step_back_context = simple_retrieval(step_back_question, docs)
    # original_context = simple_retrieval(original_question, docs)
    

    # STEP 3: Generate final answer using both questions and contexts
    final_prompt = ChatPromptTemplate.from_template(
        """I'll help you understand the topic by first considering a broader perspective.
        
        To understand "{original_question}", let's first consider this broader question:
        
        STEP-BACK QUESTION: {step_back_question}
        
        STEP-BACK CONTEXT:
        {step_back_context}
        
        Now, with this broader understanding, I can address your original question:
        
        ORIGINAL QUESTION: {original_question}
        
        ORIGINAL CONTEXT:
        {original_context}
        
        FINAL ANSWER:"""
    )
    
    final_chain = final_prompt | llm | StrOutputParser()
    response = final_chain.invoke({
        "original_question": original_question,
        "step_back_question": step_back_question,
        "step_back_context": itemgetter("step_back_context") | retriever,
        "original_context": itemgetter("original_question") | retriever,
    })
    
    # logger.info(f"Final Answer:\n{response}")
    # logger.info("=" * 60)
    

    spans = setup.get_captured_spans()
    for span in spans:
        span_attributes = span.attributes
        if "span.type" in span_attributes and span_attributes["span.type"] == "retrieval":
            # Assertions for all retrieval attributes
            assert span_attributes["entity.1.name"] == "Chroma"
            assert span_attributes["entity.1.type"] == "vectorstore.Chroma"
            assert "entity.1.deployment" in span_attributes
            assert span_attributes["entity.2.name"] == "text-embedding-ada-002"
            assert span_attributes["entity.2.type"] == "model.embedding.text-embedding-ada-002"

        if "span.type" in span_attributes and (
            span_attributes["span.type"] == "inference" or span_attributes["span.type"] == "inference.framework"):
            # Assertions for all inference attributes
            assert span_attributes["entity.1.type"] == "inference.openai"
            assert "entity.1.provider_name" in span_attributes
            assert "entity.1.inference_endpoint" in span_attributes
            assert span_attributes["entity.2.name"] == "gpt-3.5-turbo-0125"
            assert span_attributes["entity.2.type"] == "model.llm.gpt-3.5-turbo-0125"

            # Assertions for metadata
            span_input, span_output, span_metadata = span.events
            assert "completion_tokens" in span_metadata.attributes
            assert "prompt_tokens" in span_metadata.attributes
            assert "total_tokens" in span_metadata.attributes

        if not span.parent and span.name == "workflow":  # Root span
            assert span_attributes["entity.1.name"] == "ashokan_rag_step_back"
            assert span_attributes["entity.1.type"] == "workflow.langchain"



if __name__ == '__main__':
    logger.info("Starting pytest...")
    pytest.main(['-v', '-s', __file__])  # Added -s flag to show print statements and logs
    logger.info("Finished pytest.")
